{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import subprocess\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539d3b65d46c4ba3a44b03523e35358b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18366.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-11448dd48068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#command = f\"fastANI --ql temp_input.txt -r all_training_files/{rep}.fasta -o temp_out.txt -t 40\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"mash dist data/all_training_files/{rep}.fasta temp_input.txt -l -t -p 20\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcommand_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommand_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msim_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 411\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    949\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rep_dict = defaultdict(list)\n",
    "with open(\"data/clusterRes_cluster.tsv\") as clusters:\n",
    "    for rep, member in (line.rstrip().split('\\t') for line in clusters):\n",
    "        rep_dict[rep].append(member)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip\n",
    "sim_dict = defaultdict(list)\n",
    "for rep, members in tqdm(rep_dict.items(), total=len(rep_dict)):\n",
    "    if len(members) == 1:\n",
    "        sim_dict[rep].append(0)\n",
    "        continue\n",
    "    with open(\"temp_input.txt\", 'w') as temp_query:\n",
    "        temp_query.writelines((f\"data/all_training_files/{mem}.fasta\\n\" for mem in members))\n",
    "    #command = f\"fastANI --ql temp_input.txt -r all_training_files/{rep}.fasta -o temp_out.txt -t 40\"\n",
    "    command = f\"mash dist data/all_training_files/{rep}.fasta temp_input.txt -l -t -p 20\"\n",
    "    command_out = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n",
    "    for line in [line.split('\\t') for line in command_out.split('\\n') if line != \"\"][1:] :\n",
    "        sim_dict[rep].append(float(line[1]))\n",
    "        if (float(line[1]) >= .75):\n",
    "            print(rep, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip\n",
    "count = 0\n",
    "singleton_count = 0\n",
    "for rep in sim_dict:\n",
    "    count += sum(1 for sim in sim_dict[rep] if sim > 0.3)\n",
    "    if len(sim_dict[rep]) == 1:\n",
    "        singleton_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18366\n",
      "0\n",
      "236\n",
      "5.681818181818182\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Skip\n",
    "print(len(rep_dict))\n",
    "print(count)\n",
    "print(singleton_count)\n",
    "import numpy as np\n",
    "print(np.mean([len(sim_dict[rep]) for rep in sim_dict if len(sim_dict[rep]) > 1]))\n",
    "#print(max(sim_dict.items(), key=lambda x: len(x[1])))\n",
    "print(len(sim_dict[\"J7OEM\"]))\n",
    "print(len(sim_dict[\"PUKJQ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "{'BWFN4ZI7', 'TWV05PEP', 'JXDP2C4M', 'EOQAQ9X1', 'ZCU48L3S', 'JQ7Z5Q44', 'PEUBDA2B', 'AV7ONIVD', 'MV1CMX4O', 'THD393NW', '20CEB9KE', 'ULVU086L', '3TXFYNKG', '20ABQYHS', 'UXK3D4GF', '1S515B69', 'HX5NMCPJ', 'I0J54PBT', '8T12OXHS', 'H20JGHP0', '5OF7OYEA', 'WSHPKJ3H', 'S15Z6XG6', 'AS30HPUK', '10TEBWK2', '25UVYUID', 'THW6JGC7', '4M3XG8RC', 'IM2JLO1B', '9Y5EWA8O', 'IO2FYB6G', 'RFUY4U4W', 'K25LXPOI', 'I7FXTVDP', 'UH5Z524P', '216DWMG6', 'F8I0DT7Z', 'N0FDUY5E', 'RKJHZGDQ', 'O7NEA7KO', '0A4AHRCT', 'AUZNSS79', 'GZMPRX5J', 'BJKTDFN4', 'J0NVCXDJ', 'YL8AOR9Q', 'W1STLS0T', '9GDHC3D0', '303BN0Z0', 'JNU5CAOV', 'F1X6DMDH', 'GHG5MDER', 'MQKR83SM', '2CJHRNWD'}\n",
      "[('J7OEM', 3482), ('O3GQU', 1500), ('3PTDM', 1321), ('7KW5H', 794), ('48073', 678), ('BK5PO', 486), ('NMI3B', 474), ('W585R', 471), ('B6SZW', 433), ('T3CRD', 411)]\n"
     ]
    }
   ],
   "source": [
    "# Skip\n",
    "sequence_to_label = {}\n",
    "with open(\"training_labels.tsv\") as labels:\n",
    "    for line in (line.rstrip().split('\\t') for line in labels):\n",
    "        for member in line[1:]:\n",
    "            sequence_to_label[member] = line[0]\n",
    "J7OEM_labels = set(sequence_to_label[member] for member in rep_dict[\"J7OEM\"])\n",
    "print(len(J7OEM_labels))\n",
    "print(J7OEM_labels)\n",
    "print(list((x[0], len(x[1])) for x in sorted(rep_dict.items(), reverse=True, key=lambda x: len(x[1]))[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(plasmid_file, sequence_to_label, frags, labels):\n",
    "    y_str = []\n",
    "    data = []\n",
    "    cols = len(frags)\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    x_le = preprocessing.LabelEncoder()\n",
    "    x_le.fit(list(frags))\n",
    "    # x_le.fit(list(frags_seen))\n",
    "    curr_id = \"\"\n",
    "    frags_hit = []\n",
    "    hit_percent = []\n",
    "    row_num = 0\n",
    "    with open(plasmid_file) as training_handle:\n",
    "        reader = csv.DictReader(training_handle, delimiter='\\t')\n",
    "        for r, line in enumerate(reader):\n",
    "            curr_id = line[\"Query seq\"]\n",
    "            break\n",
    "    with open(plasmid_file) as training_handle:\n",
    "        reader = csv.DictReader(training_handle, delimiter='\\t')\n",
    "        for r, line in enumerate(reader):\n",
    "            seq_id = line[\"Query seq\"]\n",
    "            if (seq_id) != curr_id:\n",
    "                if len(frags_hit) == 0:\n",
    "                    print(f\"{curr_id} doesn't hit anything!\")\n",
    "                columns = x_le.transform(frags_hit)\n",
    "                row_ind.extend([row_num for _ in range(len(columns))])\n",
    "                col_ind.extend(columns)\n",
    "                data.extend(1 for _ in range(len(frags_hit)))\n",
    "                y_str.append(sequence_to_label[curr_id])\n",
    "                frags_hit = []\n",
    "                hit_percent = []\n",
    "                curr_id = seq_id\n",
    "                row_num += 1\n",
    "            #if float(line[\"%IDY\"]) > .95:\n",
    "            frags_hit.append(line[\"Frag seq\"])\n",
    "            hit_percent.append(float(line[\"%IDY\"]))\n",
    "        columns = x_le.transform(frags_hit)\n",
    "        # columns = x_le.transform(line[2:])\n",
    "        row_ind.extend([row_num for _ in range(len(columns))])\n",
    "        col_ind.extend(columns)\n",
    "        data.extend(1 for _ in range(len(frags_hit)))\n",
    "        y_str.append(sequence_to_label[curr_id])   \n",
    "    rows = row_num + 1\n",
    "    X_train_fragged = sparse.csr_matrix((data, (row_ind, col_ind)), shape=(rows, cols))\n",
    "    y_le = preprocessing.LabelEncoder()\n",
    "    y_le.fit(labels)\n",
    "    y_train_fragged = y_le.transform(y_str)\n",
    "    return X_train_fragged, y_train_fragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(preds, truths, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:,-n:]\n",
    "    successes = 0\n",
    "    for i in range(len(truths)):\n",
    "      if truths[i] in best_n[i,:]:\n",
    "        successes += 1\n",
    "    return float(successes)/len(truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_plasmidhawk_on_cluster(seq_ids, output_dir=\"phawk_run\"):\n",
    "    print(\"Running linear pangenome alignment on\", output_dir)\n",
    "    with open(\"training_labels.tsv\") as labels:\n",
    "        for line in (line.rstrip().split('\\t') for line in labels):\n",
    "            for member in line[1:]:\n",
    "                sequence_to_label[member] = line[0]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    seq_counter = Counter(y_orig)\n",
    "    seq_ids = [seq_id for seq_id in seq_ids if seq_counter[sequence_to_label[seq_id[0]]] > 1]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    if len(Counter(y_orig)) >= 9:\n",
    "        print(len(Counter(y_orig)), \"labs for\", len(y_orig), \"sequences\")\n",
    "    else: \n",
    "        print(\"Not enough labs\\n\")\n",
    "        return\n",
    "    X_train, X_test, y_train, y_test = train_test_split(seq_ids, y_orig, test_size=0.2, stratify=y_orig)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # Create training file:\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, \"training_sequences.txt\"), 'w') as training_seqs_file:\n",
    "        training_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_train)\n",
    "    with open(os.path.join(output_dir, \"testing_sequences.txt\"), 'w') as testing_seqs_file:\n",
    "        testing_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_test)\n",
    "    with open(os.path.join(output_dir, \"plaster.out\"), 'w') as plaster_out, open(os.path.join(output_dir, \"plaster.err\"), 'w') as plaster_err:\n",
    "        subprocess.check_call(\"plaster {} --realign --output {} --work-dir {} -p 40\".format(\n",
    "                os.path.join(output_dir, \"training_sequences.txt\"),\n",
    "                os.path.join(output_dir, \"plaster_train_results\"),\n",
    "                os.path.join(output_dir, \"plaster_train_work\")).split(' '),\n",
    "            stdout=plaster_out, stderr=plaster_err)\n",
    "    train_plasmid_file = os.path.join(output_dir, \"plaster_train_results.tsv\")\n",
    "    frags_seen = set()\n",
    "    with open(train_plasmid_file) as training_handle:\n",
    "        reader = csv.DictReader(training_handle, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            frags_seen.add(line[\"Frag seq\"])\n",
    "    max_frag = int(max(frags_seen, key=lambda x: int(x.split(\"_\")[1])).split(\"_\")[1])\n",
    "    frags_seen = [f\"frag_{idx}\" for idx in range(max_frag + 1)]  \n",
    "    with open(os.path.join(output_dir, \"plaster_test.out\"), 'w') as plaster_out, open(os.path.join(output_dir, \"plaster_test.err\"), 'w') as plaster_err:\n",
    "        subprocess.check_call(\"plaster {} --output {} --work-dir {} -p 40 --align-only --template {}\".format(\n",
    "                os.path.join(output_dir, \"testing_sequences.txt\"),\n",
    "                os.path.join(output_dir, \"plaster_test_results\"),\n",
    "                os.path.join(output_dir, \"plaster_test_work\"),\n",
    "                os.path.join(output_dir, \"plaster_train_results.fasta\")).split(' '),\n",
    "            stdout=plaster_out, stderr=plaster_err)\n",
    "    test_plasmid_file = os.path.join(output_dir, \"plaster_test_results.tsv\")\n",
    "    t1 = time.time()\n",
    "    print(\"Linear method pipeline took\", t1 - t0, \"seconds.\")\n",
    "    \n",
    "    X_train_fragged, y_train_fragged = get_X_y(train_plasmid_file, sequence_to_label, frags_seen, list(set(y_orig)))\n",
    "    clf = RandomForestClassifier(n_estimators=1000, n_jobs=80, min_samples_split=2, max_depth=20)\n",
    "    clf.fit(X_train_fragged, y_train_fragged)\n",
    "    y_pred = clf.predict(X_train_fragged)\n",
    "    print(\"Top 1 train accuracy\", accuracy_score(y_train_fragged, y_pred))\n",
    "    X_test_fragged, y_test_fragged = get_X_y(test_plasmid_file, sequence_to_label, frags_seen, list(set(y_orig)))\n",
    "    y_pred = clf.predict(X_test_fragged)\n",
    "    print(\"Top 1 test accuracy\", accuracy_score(y_test_fragged, y_pred))\n",
    "    y_pred = clf.predict_proba(X_test_fragged)\n",
    "    print(\"Top 5 test accuracy\", top_n_accuracy(y_pred, y_test_fragged, 5))\n",
    "    t2 = time.time()\n",
    "    print(\"Linear method machine learning took\", t2 - t1, \"seconds.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_clusters = list(x[0] for x in sorted(rep_dict.items(), reverse=True, key=lambda x: len(x[1]))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_X_y(json_obj, sequence_to_label, nodes_seen, labels):\n",
    "    y_str = []\n",
    "    data = []\n",
    "    seqs_seen = {obj[\"name\"] for obj in json_obj}\n",
    "    cols = len(nodes_seen)\n",
    "    rows = len(seqs_seen)\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    x_le = preprocessing.LabelEncoder()\n",
    "    x_le.fit(list(nodes_seen))\n",
    "\n",
    "    seq_to_nodes_hit = defaultdict(list)\n",
    "    for obj in json_obj:\n",
    "        seq_to_nodes_hit[obj[\"name\"]].extend(mapping[\"position\"][\"node_id\"] for mapping in obj[\"path\"][\"mapping\"] if \"node_id\" in mapping[\"position\"] if mapping[\"position\"][\"node_id\"] in nodes_seen)\n",
    "    for row_num, seq_id in enumerate(seq_to_nodes_hit):\n",
    "        columns = x_le.transform(seq_to_nodes_hit[seq_id])\n",
    "        row_ind.extend([row_num for _ in range(len(columns))])\n",
    "        col_ind.extend(columns)\n",
    "        data.extend(1 for _ in range(len(columns)))\n",
    "        y_str.append(sequence_to_label[seq_id])\n",
    "\n",
    "    X_train_fragged = sparse.csr_matrix((data, (row_ind, col_ind)), shape=(rows, cols))\n",
    "    y_le = preprocessing.LabelEncoder()\n",
    "    y_le.fit(labels)\n",
    "    y_train_fragged = y_le.transform(y_str)\n",
    "    return X_train_fragged, y_train_fragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_bcalm_GA_on_cluster(seq_ids, output_dir=\"bcalm_run\"):\n",
    "    print(\"Running graph alignment on\", output_dir)\n",
    "    # Prepare input for passing to bcalm + GraphAligner\n",
    "    with open(\"training_labels.tsv\") as labels:\n",
    "        for line in (line.rstrip().split('\\t') for line in labels):\n",
    "            for member in line[1:]:\n",
    "                sequence_to_label[member] = line[0]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    seq_counter = Counter(y_orig)\n",
    "    seq_ids = [seq_id for seq_id in seq_ids if seq_counter[sequence_to_label[seq_id[0]]] > 1]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    if len(Counter(y_orig)) >= 9:\n",
    "        print(len(Counter(y_orig)), \"labs for\", len(y_orig), \"sequences\")\n",
    "    else: \n",
    "        print(\"Not enough labs\\n\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(seq_ids, y_orig, test_size=0.2, stratify=y_orig)\n",
    "\n",
    "    # Create training file:\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, \"training_sequences.txt\"), 'w') as training_seqs_file:\n",
    "        training_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_train)\n",
    "    with open(os.path.join(output_dir, \"training_sequences.txt\"), 'r') as training_seqs_file, open(os.path.join(output_dir, \"training_sequences.fasta\"), 'w') as training_fasta_file:\n",
    "        for line in training_seqs_file:\n",
    "            training_fasta_file.write(open(line.strip()).read())\n",
    "    with open(os.path.join(output_dir, \"testing_sequences.txt\"), 'w') as testing_seqs_file:\n",
    "        testing_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_test)\n",
    "    with open(os.path.join(output_dir, \"testing_sequences.txt\"), 'r') as testing_seqs_file, open(os.path.join(output_dir, \"testing_sequences.fasta\"), 'w') as testing_fasta_file:\n",
    "        for line in testing_seqs_file:\n",
    "            testing_fasta_file.write(open(line.strip()).read())\n",
    "    t0 = time.time()      \n",
    "    # Run bcalm + GraphAligner pipeline      \n",
    "    Path(os.path.join(output_dir, \"bcalm_test\")).mkdir(parents=True, exist_ok=True)\n",
    "    command = f\"bcalm -in {output_dir}/training_sequences.fasta -abundance-min 1 -out {output_dir}/bcalm_test/training -nb-cores 20\"\n",
    "    with open(f\"{output_dir}/bcalm_test/bcalm.out\", 'w') as bcalm_out, open(f\"{output_dir}/bcalm_test/bcalm.err\", 'w') as bcalm_err:\n",
    "        subprocess.check_call(command.split(' '), stdout=bcalm_out, stderr=bcalm_err)\n",
    "    command = f\"convertToGFA.py {output_dir}/bcalm_test/training.unitigs.fa {output_dir}/bcalm_test/training.gfa 31\"\n",
    "    subprocess.check_call(command.split(' '))\n",
    "    \n",
    "    command = f\"GraphAligner -g {output_dir}/bcalm_test/training.gfa -f {output_dir}/training_sequences.fasta -a {output_dir}/bcalm_test/training.gam -t 20 -x dbg\"\n",
    "    with open(f\"{output_dir}/bcalm_test/galigner.out\", 'w') as galigner_out, open(f\"{output_dir}/bcalm_test/galigner.err\", 'w') as galigner_err:\n",
    "        subprocess.check_call(command.split(' '), stdout=galigner_out, stderr=galigner_err)\n",
    "    with open(f\"{output_dir}/bcalm_test/training_alignment.json\", 'w') as training_json:\n",
    "        command = f\"vg view -a {output_dir}/bcalm_test/training.gam\"\n",
    "        subprocess.check_call(command.split(' '), stdout=training_json)\n",
    "    \n",
    "    command = f\"GraphAligner -g {output_dir}/bcalm_test/training.gfa -f {output_dir}/testing_sequences.fasta -a {output_dir}/bcalm_test/testing.gam -t 20 -x dbg\"\n",
    "    with open(f\"{output_dir}/bcalm_test/galigner.out\", 'a') as galigner_out, open(f\"{output_dir}/bcalm_test/galigner.err\", 'a') as galigner_err:\n",
    "        subprocess.check_call(command.split(' '), stdout=galigner_out, stderr=galigner_err)\n",
    "    with open(f\"{output_dir}/bcalm_test/testing_alignment.json\", 'w') as testing_json:\n",
    "        command = f\"vg view -a {output_dir}/bcalm_test/testing.gam\"\n",
    "        subprocess.check_call(command.split(' '), stdout=testing_json)\n",
    "    t1 = time.time()\n",
    "    print(\"Graph method pipeline took\", t1 - t0, \"seconds.\")   \n",
    "    # Parse Output\n",
    "    train_json_obj = []\n",
    "    with open(f\"{output_dir}/bcalm_test/training_alignment.json\") as input_json:\n",
    "        for line in input_json:\n",
    "            train_json_obj.append(json.loads(line))\n",
    "    test_json_obj = []\n",
    "    with open(f\"{output_dir}/bcalm_test/testing_alignment.json\") as input_json:\n",
    "        for line in input_json:\n",
    "            test_json_obj.append(json.loads(line))\n",
    "            \n",
    "    nodes_seen = set()\n",
    "    for obj in train_json_obj:\n",
    "#         print(obj[\"name\"], len(obj[\"path\"][\"mapping\"]))\n",
    "        for mapping in obj[\"path\"][\"mapping\"]:\n",
    "            if \"node_id\" not in mapping[\"position\"]:\n",
    "                continue\n",
    "            nodes_seen.add(mapping[\"position\"][\"node_id\"])\n",
    "    seqs_seen = {obj[\"name\"] for obj in train_json_obj}\n",
    "    X_train_fragged, y_train_fragged = get_graph_X_y(train_json_obj, sequence_to_label, nodes_seen, list(set(y_orig)))\n",
    "    clf = RandomForestClassifier(n_estimators=1000, n_jobs=80, min_samples_split=2, max_depth=20)\n",
    "    clf.fit(X_train_fragged, y_train_fragged)\n",
    "    y_pred = clf.predict(X_train_fragged)\n",
    "    print(\"Top 1 train accuracy\", accuracy_score(y_train_fragged, y_pred))\n",
    "    X_test_fragged, y_test_fragged = get_graph_X_y(test_json_obj, sequence_to_label, nodes_seen, list(set(y_orig)))\n",
    "    y_pred = clf.predict(X_test_fragged)\n",
    "    print(\"Top 1 test accuracy\", accuracy_score(y_test_fragged, y_pred))\n",
    "    y_pred = clf.predict_proba(X_test_fragged)\n",
    "    print(\"Top 5 test accuracy\", top_n_accuracy(y_pred, y_test_fragged, 5))\n",
    "    t2 = time.time()\n",
    "    print(\"Graph method machine learning took\", t2 - t1, \"seconds.\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running graph alignment on O3GQU\n",
      "131 labs for 1440 sequences\n",
      "Graph method pipeline took 8.851826190948486 seconds.\n",
      "Top 1 train accuracy 0.9444444444444444\n",
      "Top 1 test accuracy 0.6597222222222222\n",
      "Top 5 test accuracy 0.8576388888888888\n",
      "Graph method machine learning took 6.7981672286987305 seconds.\n",
      "\n",
      "Running linear pangenome alignment on O3GQU\n",
      "131 labs for 1440 sequences\n",
      "Linear method pipeline took 498.57411909103394 seconds.\n",
      "Top 1 train accuracy 0.5633680555555556\n",
      "Top 1 test accuracy 0.4131944444444444\n",
      "Top 5 test accuracy 0.7777777777777778\n",
      "Linear method machine learning took 3.4982948303222656 seconds.\n",
      "\n",
      "Running graph alignment on 3PTDM\n",
      "135 labs for 1238 sequences\n",
      "Graph method pipeline took 8.84386420249939 seconds.\n",
      "Top 1 train accuracy 0.9484848484848485\n",
      "Top 1 test accuracy 0.7983870967741935\n",
      "Top 5 test accuracy 0.9475806451612904\n",
      "Graph method machine learning took 5.7371132373809814 seconds.\n",
      "\n",
      "Running linear pangenome alignment on 3PTDM\n",
      "135 labs for 1238 sequences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1fb52170cbe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcluster_rep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_n_clusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrun_bcalm_GA_on_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_rep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrun_plasmidhawk_on_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcluster_rep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcluster_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7f4d838afb7f>\u001b[0m in \u001b[0;36mrun_plasmidhawk_on_cluster\u001b[0;34m(seq_ids, output_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"plaster_train_results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 os.path.join(output_dir, \"plaster_train_work\")).split(' '),\n\u001b[0;32m---> 30\u001b[0;31m             stdout=plaster_out, stderr=plaster_err)\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_plasmid_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"plaster_train_results.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mfrags_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/software/miniconda3/envs/ML/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1609\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "top_n_clusters = list(x[0] for x in sorted(rep_dict.items(), reverse=True, key=lambda x: len(x[1]))[:20])\n",
    "for cluster_rep in top_n_clusters[1:15]:\n",
    "    run_bcalm_GA_on_cluster([[seq_id] for seq_id in rep_dict[cluster_rep]], output_dir = cluster_rep)  \n",
    "    run_plasmidhawk_on_cluster([[seq_id] for seq_id in rep_dict[cluster_rep]], output_dir = cluster_rep)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minigraph alone does not create any useful graph ): \n",
    "for cluster_rep in top_n_clusters[1:10]:\n",
    "    ref_file = f\"all_training_files/{cluster_rep}.fasta\"\n",
    "    for member in rep_dict[cluster_rep]:\n",
    "        if member == cluster_rep:\n",
    "            continue\n",
    "        member_file = f\"data/all_training_files/{member}.fasta\"\n",
    "        with open(f\"{cluster_rep}/minigraph_out_tmp.gfa\", 'w') as minigraph_out, open(f\"{cluster_rep}/minigraph.err\", 'w') as minigraph_err:\n",
    "            command = f\"minigraph -x ggs {ref_file} {member_file}\"\n",
    "            print(command, \"> \")\n",
    "            subprocess.check_call(command.split(' '), stdout=minigraph_out, stderr=minigraph_err)\n",
    "        shutil.copyfile(f\"{cluster_rep}/minigraph_out_tmp.gfa\", f\"{cluster_rep}/minigraph_out.gfa\")\n",
    "        ref_file = f\"{cluster_rep}/minigraph_out.gfa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out nucdif for SVs. Gives .gff files and not sure if these are what we're looking for\n",
    "for cluster_rep in top_n_clusters[1:10]:\n",
    "    ref_file = f\"all_training_files/{cluster_rep}.fasta\"\n",
    "    for member in rep_dict[cluster_rep]:\n",
    "        if member == cluster_rep:\n",
    "            continue\n",
    "        member_file = f\"data/all_training_files/{member}.fasta\"\n",
    "        with open(f\"{cluster_rep}/minigraph_out_tmp.gfa\", 'w') as minigraph_out, open(f\"{cluster_rep}/minigraph.err\", 'w') as minigraph_err:\n",
    "            command = f\"nucdiff --vcf yes {ref_file} {member_file} {cluster_rep}/nd_out/{member} nd_nuc\"\n",
    "            subprocess.check_call(command.split(' '), stdout=minigraph_out, stderr=minigraph_err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
