{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import subprocess\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_dict = defaultdict(list)\n",
    "with open(\"data/clusterRes_cluster.tsv\") as clusters:\n",
    "    for rep, member in (line.rstrip().split('\\t') for line in clusters):\n",
    "        rep_dict[rep].append(member)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip\n",
    "sim_dict = defaultdict(list)\n",
    "for rep, members in tqdm(rep_dict.items(), total=len(rep_dict)):\n",
    "    if len(members) == 1:\n",
    "        sim_dict[rep].append(0)\n",
    "        continue\n",
    "    with open(\"temp_input.txt\", 'w') as temp_query:\n",
    "        temp_query.writelines((f\"data/all_training_files/{mem}.fasta\\n\" for mem in members))\n",
    "    #command = f\"fastANI --ql temp_input.txt -r all_training_files/{rep}.fasta -o temp_out.txt -t 40\"\n",
    "    command = f\"mash dist data/all_training_files/{rep}.fasta temp_input.txt -l -t -p 20\"\n",
    "    command_out = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n",
    "    for line in [line.split('\\t') for line in command_out.split('\\n') if line != \"\"][1:] :\n",
    "        sim_dict[rep].append(float(line[1]))\n",
    "        if (float(line[1]) >= .75):\n",
    "            print(rep, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip\n",
    "count = 0\n",
    "singleton_count = 0\n",
    "for rep in sim_dict:\n",
    "    count += sum(1 for sim in sim_dict[rep] if sim > 0.3)\n",
    "    if len(sim_dict[rep]) == 1:\n",
    "        singleton_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip\n",
    "print(len(rep_dict))\n",
    "print(count)\n",
    "print(singleton_count)\n",
    "import numpy as np\n",
    "print(np.mean([len(sim_dict[rep]) for rep in sim_dict if len(sim_dict[rep]) > 1]))\n",
    "#print(max(sim_dict.items(), key=lambda x: len(x[1])))\n",
    "print(len(sim_dict[\"J7OEM\"]))\n",
    "print(len(sim_dict[\"PUKJQ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip\n",
    "sequence_to_label = {}\n",
    "with open(\"training_labels.tsv\") as labels:\n",
    "    for line in (line.rstrip().split('\\t') for line in labels):\n",
    "        for member in line[1:]:\n",
    "            sequence_to_label[member] = line[0]\n",
    "J7OEM_labels = set(sequence_to_label[member] for member in rep_dict[\"J7OEM\"])\n",
    "print(len(J7OEM_labels))\n",
    "print(J7OEM_labels)\n",
    "print(list((x[0], len(x[1])) for x in sorted(rep_dict.items(), reverse=True, key=lambda x: len(x[1]))[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(plasmid_file, sequence_to_label, frags, labels):\n",
    "    y_str = []\n",
    "    data = []\n",
    "    cols = len(frags)\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    x_le = preprocessing.LabelEncoder()\n",
    "    x_le.fit(list(frags))\n",
    "    # x_le.fit(list(frags_seen))\n",
    "    curr_id = \"\"\n",
    "    frags_hit = []\n",
    "    hit_percent = []\n",
    "    row_num = 0\n",
    "    with open(plasmid_file) as training_handle:\n",
    "        reader = csv.DictReader(training_handle, delimiter='\\t')\n",
    "        for r, line in enumerate(reader):\n",
    "            curr_id = line[\"Query seq\"]\n",
    "            break\n",
    "    with open(plasmid_file) as training_handle:\n",
    "        reader = csv.DictReader(training_handle, delimiter='\\t')\n",
    "        for r, line in enumerate(reader):\n",
    "            seq_id = line[\"Query seq\"]\n",
    "            if (seq_id) != curr_id:\n",
    "                if len(frags_hit) == 0:\n",
    "                    print(f\"{curr_id} doesn't hit anything!\")\n",
    "                columns = x_le.transform(frags_hit)\n",
    "                row_ind.extend([row_num for _ in range(len(columns))])\n",
    "                col_ind.extend(columns)\n",
    "                data.extend(1 for _ in range(len(frags_hit)))\n",
    "                y_str.append(sequence_to_label[curr_id])\n",
    "                frags_hit = []\n",
    "                hit_percent = []\n",
    "                curr_id = seq_id\n",
    "                row_num += 1\n",
    "            #if float(line[\"%IDY\"]) > .95:\n",
    "            frags_hit.append(line[\"Frag seq\"])\n",
    "            hit_percent.append(float(line[\"%IDY\"]))\n",
    "        columns = x_le.transform(frags_hit)\n",
    "        # columns = x_le.transform(line[2:])\n",
    "        row_ind.extend([row_num for _ in range(len(columns))])\n",
    "        col_ind.extend(columns)\n",
    "        data.extend(1 for _ in range(len(frags_hit)))\n",
    "        y_str.append(sequence_to_label[curr_id])   \n",
    "    rows = row_num + 1\n",
    "    X_train_fragged = sparse.csr_matrix((data, (row_ind, col_ind)), shape=(rows, cols))\n",
    "    y_le = preprocessing.LabelEncoder()\n",
    "    y_le.fit(labels)\n",
    "    y_train_fragged = y_le.transform(y_str)\n",
    "    return X_train_fragged, y_train_fragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(preds, truths, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:,-n:]\n",
    "    successes = 0\n",
    "    for i in range(len(truths)):\n",
    "      if truths[i] in best_n[i,:]:\n",
    "        successes += 1\n",
    "    return float(successes)/len(truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_plasmidhawk_on_cluster(seq_ids, output_dir=\"phawk_run\"):\n",
    "    print(\"Running linear pangenome alignment on\", output_dir)\n",
    "    with open(\"training_labels.tsv\") as labels:\n",
    "        for line in (line.rstrip().split('\\t') for line in labels):\n",
    "            for member in line[1:]:\n",
    "                sequence_to_label[member] = line[0]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    seq_counter = Counter(y_orig)\n",
    "    seq_ids = [seq_id for seq_id in seq_ids if seq_counter[sequence_to_label[seq_id[0]]] > 1]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    if len(Counter(y_orig)) >= 9:\n",
    "        print(len(Counter(y_orig)), \"labs for\", len(y_orig), \"sequences\")\n",
    "    else: \n",
    "        print(\"Not enough labs\\n\")\n",
    "        return\n",
    "    X_train, X_test, y_train, y_test = train_test_split(seq_ids, y_orig, test_size=0.2, stratify=y_orig)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    # Create training file:\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, \"training_sequences.txt\"), 'w') as training_seqs_file:\n",
    "        training_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_train)\n",
    "    with open(os.path.join(output_dir, \"testing_sequences.txt\"), 'w') as testing_seqs_file:\n",
    "        testing_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_test)\n",
    "    with open(os.path.join(output_dir, \"plaster.out\"), 'w') as plaster_out, open(os.path.join(output_dir, \"plaster.err\"), 'w') as plaster_err:\n",
    "        subprocess.check_call(\"plaster {} --realign --output {} --work-dir {} -p 40\".format(\n",
    "                os.path.join(output_dir, \"training_sequences.txt\"),\n",
    "                os.path.join(output_dir, \"plaster_train_results\"),\n",
    "                os.path.join(output_dir, \"plaster_train_work\")).split(' '),\n",
    "            stdout=plaster_out, stderr=plaster_err)\n",
    "    train_plasmid_file = os.path.join(output_dir, \"plaster_train_results.tsv\")\n",
    "    frags_seen = set()\n",
    "    with open(train_plasmid_file) as training_handle:\n",
    "        reader = csv.DictReader(training_handle, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            frags_seen.add(line[\"Frag seq\"])\n",
    "    max_frag = int(max(frags_seen, key=lambda x: int(x.split(\"_\")[1])).split(\"_\")[1])\n",
    "    frags_seen = [f\"frag_{idx}\" for idx in range(max_frag + 1)]  \n",
    "    with open(os.path.join(output_dir, \"plaster_test.out\"), 'w') as plaster_out, open(os.path.join(output_dir, \"plaster_test.err\"), 'w') as plaster_err:\n",
    "        subprocess.check_call(\"plaster {} --output {} --work-dir {} -p 40 --align-only --template {}\".format(\n",
    "                os.path.join(output_dir, \"testing_sequences.txt\"),\n",
    "                os.path.join(output_dir, \"plaster_test_results\"),\n",
    "                os.path.join(output_dir, \"plaster_test_work\"),\n",
    "                os.path.join(output_dir, \"plaster_train_results.fasta\")).split(' '),\n",
    "            stdout=plaster_out, stderr=plaster_err)\n",
    "    test_plasmid_file = os.path.join(output_dir, \"plaster_test_results.tsv\")\n",
    "    t1 = time.time()\n",
    "    print(\"Linear method pipeline took\", t1 - t0, \"seconds.\")\n",
    "    \n",
    "    X_train_fragged, y_train_fragged = get_X_y(train_plasmid_file, sequence_to_label, frags_seen, list(set(y_orig)))\n",
    "    clf = RandomForestClassifier(n_estimators=1000, n_jobs=80, min_samples_split=2, max_depth=20)\n",
    "    clf.fit(X_train_fragged, y_train_fragged)\n",
    "    y_pred = clf.predict(X_train_fragged)\n",
    "    print(\"Top 1 train accuracy\", accuracy_score(y_train_fragged, y_pred))\n",
    "    X_test_fragged, y_test_fragged = get_X_y(test_plasmid_file, sequence_to_label, frags_seen, list(set(y_orig)))\n",
    "    y_pred = clf.predict(X_test_fragged)\n",
    "    print(\"Top 1 test accuracy\", accuracy_score(y_test_fragged, y_pred))\n",
    "    y_pred = clf.predict_proba(X_test_fragged)\n",
    "    print(\"Top 5 test accuracy\", top_n_accuracy(y_pred, y_test_fragged, 5))\n",
    "    t2 = time.time()\n",
    "    print(\"Linear method machine learning took\", t2 - t1, \"seconds.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_clusters = list(x[0] for x in sorted(rep_dict.items(), reverse=True, key=lambda x: len(x[1]))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_X_y(json_obj, sequence_to_label, nodes_seen, labels):\n",
    "    y_str = []\n",
    "    data = []\n",
    "    seqs_seen = {obj[\"name\"] for obj in json_obj}\n",
    "    cols = len(nodes_seen)\n",
    "    rows = len(seqs_seen)\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    x_le = preprocessing.LabelEncoder()\n",
    "    x_le.fit(list(nodes_seen))\n",
    "\n",
    "    seq_to_nodes_hit = defaultdict(list)\n",
    "    for obj in json_obj:\n",
    "        seq_to_nodes_hit[obj[\"name\"]].extend(mapping[\"position\"][\"node_id\"] for mapping in obj[\"path\"][\"mapping\"] if \"node_id\" in mapping[\"position\"] if mapping[\"position\"][\"node_id\"] in nodes_seen)\n",
    "    for row_num, seq_id in enumerate(seq_to_nodes_hit):\n",
    "        columns = x_le.transform(seq_to_nodes_hit[seq_id])\n",
    "        row_ind.extend([row_num for _ in range(len(columns))])\n",
    "        col_ind.extend(columns)\n",
    "        data.extend(1 for _ in range(len(columns)))\n",
    "        y_str.append(sequence_to_label[seq_id])\n",
    "\n",
    "    X_train_fragged = sparse.csr_matrix((data, (row_ind, col_ind)), shape=(rows, cols))\n",
    "    y_le = preprocessing.LabelEncoder()\n",
    "    y_le.fit(labels)\n",
    "    y_train_fragged = y_le.transform(y_str)\n",
    "    return X_train_fragged, y_train_fragged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_bcalm_GA_on_cluster(seq_ids, output_dir=\"bcalm_run\"):\n",
    "    print(\"Running graph alignment on\", output_dir)\n",
    "    # Prepare input for passing to bcalm + GraphAligner\n",
    "    with open(\"training_labels.tsv\") as labels:\n",
    "        for line in (line.rstrip().split('\\t') for line in labels):\n",
    "            for member in line[1:]:\n",
    "                sequence_to_label[member] = line[0]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    seq_counter = Counter(y_orig)\n",
    "    seq_ids = [seq_id for seq_id in seq_ids if seq_counter[sequence_to_label[seq_id[0]]] > 1]\n",
    "    y_orig = [sequence_to_label[mem[0]] for mem in seq_ids]\n",
    "    if len(Counter(y_orig)) >= 9:\n",
    "        print(len(Counter(y_orig)), \"labs for\", len(y_orig), \"sequences\")\n",
    "    else: \n",
    "        print(\"Not enough labs\\n\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(seq_ids, y_orig, test_size=0.2, stratify=y_orig)\n",
    "\n",
    "    # Create training file:\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, \"training_sequences.txt\"), 'w') as training_seqs_file:\n",
    "        training_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_train)\n",
    "    with open(os.path.join(output_dir, \"training_sequences.txt\"), 'r') as training_seqs_file, open(os.path.join(output_dir, \"training_sequences.fasta\"), 'w') as training_fasta_file:\n",
    "        for line in training_seqs_file:\n",
    "            training_fasta_file.write(open(line.strip()).read())\n",
    "    with open(os.path.join(output_dir, \"testing_sequences.txt\"), 'w') as testing_seqs_file:\n",
    "        testing_seqs_file.writelines(f\"data/all_training_files/{seq_id[0]}.fasta\\n\" for seq_id in X_test)\n",
    "    with open(os.path.join(output_dir, \"testing_sequences.txt\"), 'r') as testing_seqs_file, open(os.path.join(output_dir, \"testing_sequences.fasta\"), 'w') as testing_fasta_file:\n",
    "        for line in testing_seqs_file:\n",
    "            testing_fasta_file.write(open(line.strip()).read())\n",
    "    t0 = time.time()      \n",
    "    # Run bcalm + GraphAligner pipeline      \n",
    "    Path(os.path.join(output_dir, \"bcalm_test\")).mkdir(parents=True, exist_ok=True)\n",
    "    command = f\"bcalm -in {output_dir}/training_sequences.fasta -abundance-min 1 -out {output_dir}/bcalm_test/training -nb-cores 20\"\n",
    "    with open(f\"{output_dir}/bcalm_test/bcalm.out\", 'w') as bcalm_out, open(f\"{output_dir}/bcalm_test/bcalm.err\", 'w') as bcalm_err:\n",
    "        subprocess.check_call(command.split(' '), stdout=bcalm_out, stderr=bcalm_err)\n",
    "    command = f\"convertToGFA.py {output_dir}/bcalm_test/training.unitigs.fa {output_dir}/bcalm_test/training.gfa 31\"\n",
    "    subprocess.check_call(command.split(' '))\n",
    "    \n",
    "    command = f\"GraphAligner -g {output_dir}/bcalm_test/training.gfa -f {output_dir}/training_sequences.fasta -a {output_dir}/bcalm_test/training.gam -t 20 -x dbg\"\n",
    "    with open(f\"{output_dir}/bcalm_test/galigner.out\", 'w') as galigner_out, open(f\"{output_dir}/bcalm_test/galigner.err\", 'w') as galigner_err:\n",
    "        subprocess.check_call(command.split(' '), stdout=galigner_out, stderr=galigner_err)\n",
    "    with open(f\"{output_dir}/bcalm_test/training_alignment.json\", 'w') as training_json:\n",
    "        command = f\"vg view -a {output_dir}/bcalm_test/training.gam\"\n",
    "        subprocess.check_call(command.split(' '), stdout=training_json)\n",
    "    \n",
    "    command = f\"GraphAligner -g {output_dir}/bcalm_test/training.gfa -f {output_dir}/testing_sequences.fasta -a {output_dir}/bcalm_test/testing.gam -t 20 -x dbg\"\n",
    "    with open(f\"{output_dir}/bcalm_test/galigner.out\", 'a') as galigner_out, open(f\"{output_dir}/bcalm_test/galigner.err\", 'a') as galigner_err:\n",
    "        subprocess.check_call(command.split(' '), stdout=galigner_out, stderr=galigner_err)\n",
    "    with open(f\"{output_dir}/bcalm_test/testing_alignment.json\", 'w') as testing_json:\n",
    "        command = f\"vg view -a {output_dir}/bcalm_test/testing.gam\"\n",
    "        subprocess.check_call(command.split(' '), stdout=testing_json)\n",
    "    t1 = time.time()\n",
    "    print(\"Graph method pipeline took\", t1 - t0, \"seconds.\")   \n",
    "    # Parse Output\n",
    "    train_json_obj = []\n",
    "    with open(f\"{output_dir}/bcalm_test/training_alignment.json\") as input_json:\n",
    "        for line in input_json:\n",
    "            train_json_obj.append(json.loads(line))\n",
    "    test_json_obj = []\n",
    "    with open(f\"{output_dir}/bcalm_test/testing_alignment.json\") as input_json:\n",
    "        for line in input_json:\n",
    "            test_json_obj.append(json.loads(line))\n",
    "            \n",
    "    nodes_seen = set()\n",
    "    for obj in train_json_obj:\n",
    "#         print(obj[\"name\"], len(obj[\"path\"][\"mapping\"]))\n",
    "        for mapping in obj[\"path\"][\"mapping\"]:\n",
    "            if \"node_id\" not in mapping[\"position\"]:\n",
    "                continue\n",
    "            nodes_seen.add(mapping[\"position\"][\"node_id\"])\n",
    "    seqs_seen = {obj[\"name\"] for obj in train_json_obj}\n",
    "    X_train_fragged, y_train_fragged = get_graph_X_y(train_json_obj, sequence_to_label, nodes_seen, list(set(y_orig)))\n",
    "    clf = RandomForestClassifier(n_estimators=1000, n_jobs=80, min_samples_split=2, max_depth=20)\n",
    "    clf.fit(X_train_fragged, y_train_fragged)\n",
    "    y_pred = clf.predict(X_train_fragged)\n",
    "    print(\"Top 1 train accuracy\", accuracy_score(y_train_fragged, y_pred))\n",
    "    X_test_fragged, y_test_fragged = get_graph_X_y(test_json_obj, sequence_to_label, nodes_seen, list(set(y_orig)))\n",
    "    y_pred = clf.predict(X_test_fragged)\n",
    "    print(\"Top 1 test accuracy\", accuracy_score(y_test_fragged, y_pred))\n",
    "    y_pred = clf.predict_proba(X_test_fragged)\n",
    "    print(\"Top 5 test accuracy\", top_n_accuracy(y_pred, y_test_fragged, 5))\n",
    "    t2 = time.time()\n",
    "    print(\"Graph method machine learning took\", t2 - t1, \"seconds.\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_clusters = list(x[0] for x in sorted(rep_dict.items(), reverse=True, key=lambda x: len(x[1]))[:20])\n",
    "for cluster_rep in top_n_clusters[1:15]:\n",
    "    run_bcalm_GA_on_cluster([[seq_id] for seq_id in rep_dict[cluster_rep]], output_dir = cluster_rep)  \n",
    "    run_plasmidhawk_on_cluster([[seq_id] for seq_id in rep_dict[cluster_rep]], output_dir = cluster_rep)  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minigraph alone does not create any useful graph ): \n",
    "for cluster_rep in top_n_clusters[1:10]:\n",
    "    ref_file = f\"all_training_files/{cluster_rep}.fasta\"\n",
    "    for member in rep_dict[cluster_rep]:\n",
    "        if member == cluster_rep:\n",
    "            continue\n",
    "        member_file = f\"data/all_training_files/{member}.fasta\"\n",
    "        with open(f\"{cluster_rep}/minigraph_out_tmp.gfa\", 'w') as minigraph_out, open(f\"{cluster_rep}/minigraph.err\", 'w') as minigraph_err:\n",
    "            command = f\"minigraph -x ggs {ref_file} {member_file}\"\n",
    "            print(command, \"> \")\n",
    "            subprocess.check_call(command.split(' '), stdout=minigraph_out, stderr=minigraph_err)\n",
    "        shutil.copyfile(f\"{cluster_rep}/minigraph_out_tmp.gfa\", f\"{cluster_rep}/minigraph_out.gfa\")\n",
    "        ref_file = f\"{cluster_rep}/minigraph_out.gfa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out nucdif for SVs. Gives .gff files and not sure if these are what we're looking for\n",
    "for cluster_rep in top_n_clusters[1:10]:\n",
    "    ref_file = f\"all_training_files/{cluster_rep}.fasta\"\n",
    "    for member in rep_dict[cluster_rep]:\n",
    "        if member == cluster_rep:\n",
    "            continue\n",
    "        member_file = f\"data/all_training_files/{member}.fasta\"\n",
    "        with open(f\"{cluster_rep}/minigraph_out_tmp.gfa\", 'w') as minigraph_out, open(f\"{cluster_rep}/minigraph.err\", 'w') as minigraph_err:\n",
    "            command = f\"nucdiff --vcf yes {ref_file} {member_file} {cluster_rep}/nd_out/{member} nd_nuc\"\n",
    "            subprocess.check_call(command.split(' '), stdout=minigraph_out, stderr=minigraph_err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
